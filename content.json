{"posts":[{"title":"Django Channels 异步操作问题笔记","text":"Django Channels 异步缓存操作错误解决方案问题描述在 Django Channels 的 WebSocket 消费者中调用缓存操作时出现以下错误：获取在线用户列表错误: You cannot call this from an async context - use a thread or sync_to_async. 问题根源1. 异步/同步上下文冲突 Django 的默认缓存操作（cache.get/cache.set）是同步操作 WebSocket 消费者继承自 AsyncWebsocketConsumer，运行在异步事件循环中 直接混用会导致事件循环阻塞，引发 SynchronousOnlyOperation 异常 2. 常见触发场景1234567891011121314151617181920212223242526# 错误示例：在异步方法中直接调用同步缓存async def get_online_users(self): online_users = cache.get(self.room_group_name) # 此处会报错---## 解决方案### 使用异步装饰器封装```pythonfrom channels.db import database_sync_to_asyncclass ChatConsumer(AsyncWebsocketConsumer): # 同步操作转异步 @database_sync_to_async def _get_cache(self, key): return cache.get(key, []) @database_sync_to_async def _set_cache(self, key, value): return cache.set(key, value, timeout=None) # 异步方法调用 async def get_online_users(self): online_users = await self._get_cache(self.room_group_name) await self._set_cache(self.room_group_name, updated_users) 关键技术要点 要点 说明 示例 异步装饰器 将同步方法转换为协程 @database_sync_to_async await 调用 必须等待异步操作完成 await self._get_cache() 上下文隔离 禁止在异步方法中直接调用同步 I/O 直接使用 cache.get()","link":"/2025/03/31/Django-Channels-%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C%E9%97%AE%E9%A2%98%E7%AC%94%E8%AE%B0/"},{"title":"","text":"线性神经网络一、线性回归1.基本元素​ 用一个例子来解释线性回归：希望根据房屋的面积和年龄来估算它的价格。为了开发出一个好用的房价预测模型，我们需要收集一个真实的数据集。这个数据集包括房屋的销售价格、面积和年龄。在机器学习的术语中，该数据集被称为训练数据集。每行数据被称为样本或数据点。试图预测的目标被称为标签(label)或者目标(target)。预测所依据的自变量被称为特征(feature)或协变量（covariate）。 线性模型​ 目标可以表现为特征的加权和，例如此式：$ \\text{price} = w_{area} \\cdot area + w_{age} \\cdot age + b. $ 给定一个数据集，我们的目标是找到模型的权重$w$和偏置$b$，使得根据模型做出的预测大体符合数据中的真实价格。​ 将所有特征放到向量$\\mathbf{x} \\in \\mathbb{R}^d$中， 并将所有权重放到向量$\\mathbf{w} \\in \\mathbb{R}^d$中， 我们可以用点积形式来简洁地表达模型：$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$$​ 对于特征集合$X$，预测值$\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ 可以通过矩阵-向量乘法表示为：$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b$$​ 既然要寻找最好的参数$w$和$b$，就需要模型质量的度量方法和能够更新模型使其质量更好的方法。 损失函数​ 损失函数用于量化目标的预测值和实际值之间的差距。通常选择非负数作为损失，且数值越小表示损失越小，完全符合时损失为0。回归问题中常使用平方误差函数作为损失函数：$$ \\mathcal{l}(w,b) = \\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$ 常数$\\frac{1}{2}$的作用是使得损失函数的一阶导数常项系数为1。 为了度量模型对整个数据集的预测表现，应该计算损失均值：$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$​ 所求参数$w$和$b$应该能最小化在此训练集的总损失：$$\\mathbf{w}^, b^ = \\operatorname*{argmin}_{\\mathbf{w}, b}\\ L(\\mathbf{w}, b).$$ 解析解​ 线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。首先，将偏置$b$合并到参数w中，合并方法是在包含所有参数的矩阵中附加一列。我们的预测问题是最小化$|\\mathbf{y} - \\mathbf{X}\\mathbf{w}|^2$。将损失关于$w$的导数设为0，得到解析解：$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$$推导过程： ​ 首先将求和式改写为矩阵乘法：$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m \\left( \\mathbf{w}^\\top \\mathbf{x}^{(i)} - y^{(i)} \\right)^2$$​ 变为：$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2} | \\mathbf{X}\\mathbf{w} - \\mathbf{y} |^2$$​ 根据L2范数的性质有：$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top (\\mathbf{X}\\mathbf{w} - \\mathbf{y})$$​ 展开为：$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2} \\left(\\underbrace{\\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}}{\\text{二次项}} -\\underbrace{2\\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{y}}{\\text{线性项}} +\\underbrace{\\mathbf{y}^\\top \\mathbf{y}}{\\text{常数项}}\\right)$$​ 求梯度：$$\\nabla{\\mathbf{w}} \\mathcal{L} = \\frac{1}{2} \\left(2\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} - 2\\mathbf{X}^\\top \\mathbf{y}\\right) = \\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} - \\mathbf{X}^\\top \\mathbf{y}$$​ 令梯度为0得到闭式解：$$\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} = \\mathbf{X}^\\top \\mathbf{y} \\\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$ 随机梯度下降​ 梯度下降算法几乎可以优化所有深度学习模型。梯度下降算法最简单的用法是计算损失函数关于模型参数的导数。但如果数据集很大那么每次更新参数的代价会很大，所以通常在每次计算更新时随机抽取一小批样本，这种变体称为小批量随机梯度下降。 二、softmax回归​ one-hot encoding（独热编码）是一种简单的表示分类数据的方法。独热编码是一个向量，具有与类别一样多的分量，类别对应的分量设为1，其他分量设为0。比如有三个类别鸡、鸭、狗，使用独热编码就是$$$y \\in {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.$$$ 网络架构​ 为了估计所有类别的可能概率，我们需要一个有多个输出的模型，每个类别对应一个输出。假设我们有4个特征和3个可能的输出类别，就需要12个标量来表示权重，三个标量来表示偏置，例如这样：$$\\begin{split}\\begin{aligned}o_1 &amp;= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\o_2 &amp;= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\o_3 &amp;= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\\end{aligned}\\end{split}$$​ 与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出取决于所有输入， 所以softmax回归的输出层也是全连接层。（什么是全连接层？在全连接层中，当前层的每一个神经元都和前一层的每一个神经元相连。也就是说信息会从每一个神经元流过，没有遗漏。全连接层又称密集层或线性层） softmax运算​ 我们希望模型的输出$\\hat{y}_j$可以视为属于类$j$的概率，然后选择具有最大输出值的$\\operatorname*{argmax}_j y_j$类别作为我们的预测。为了保证模型输出的结果是一个在0到1之间的概率，我们需要softmax函数。​ softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{其中}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$$","link":"/2025/07/03/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"多远都要在一起","text":"42 想听你听过的音乐 想看你看过的小说 我想收集每一刻 我想看到你眼中的世界 想到你到过的地方 和你曾度过的时光 不想错过每一刻 多希望我一直在你身旁 未来何从何去 你快乐我也就没关系 对你我最熟悉 你爱自由我却更爱你 我能习惯远距离 爱总是身不由己 宁愿换个方式至少还能遥远爱着你 爱能克服远距离 多远都要在一起 你已经不再存在我的世界里 请不要离开我的回忆 想你说爱我的语气 想你望着我的眼睛 不想忘记每一刻 用思念让我们一直前进 想像你失落的唇印 想象你失约的旅行 想象你离开的那一刻 如果我有留下你的勇气 我能习惯远距离 爱总是身不由己 宁愿换个方式至少还能遥远爱着你 爱能克服远距离 多远都要在一起 我已经不再存在你的心里 就让我独自守着回忆 如果阳光永远都炙热 如果彩虹不会掉颜色 你能不能不要离开呢 我能习惯远距离 爱总是身不由己 宁愿换个方式至少还能遥远爱着你 爱能克服远距离 多远都要在一起 你已经不再存在我的世界里 请不要离开我回忆 请不要离开不要离开我的回忆 var ap = new APlayer({ element: document.getElementById(\"aplayer-UFJtjKbu\"), narrow: false, autoplay: true, showlrc: false, music: { title: \"多远都要在一起\", author: \"邓紫棋\", url: \"/2025/02/19/你已经不再存在我的世界里/多远都要在一起-G.E.M.邓紫棋.mp3\", pic: \"\", lrc: \"\" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap);","link":"/2025/02/19/%E4%BD%A0%E5%B7%B2%E7%BB%8F%E4%B8%8D%E5%86%8D%E5%AD%98%E5%9C%A8%E6%88%91%E7%9A%84%E4%B8%96%E7%95%8C%E9%87%8C/"}],"tags":[],"categories":[],"pages":[]}